{"componentChunkName":"component---src-templates-notes-template-js","path":"/notes/microservices/mlflow_on_kube/","result":{"data":{"markdownRemark":{"html":"<!-- ## Run Container for mlflow models\n### Create Image\n```\nmlflow models build-docker --m \"runs:/{run_id}/my-model\" --name {image_name} \n```\n### Run Container\n```\ndocker run -p {local_port}:{image_port} {image_name}\n``` -->\n<h2>Local Environment</h2>\n<pre><code>kind create cluster --name {cluster_name}\n</code></pre>\n<h2>Set up Seldon Core</h2>\n<pre><code>kubectl create namespace {namespace}\n\nhelm install seldon-core seldon-core-operator \\\n    --repo https://storage.googleapis.com/seldon-charts \\\n    --set usageMetrics.enabled=true \\\n    --namespace {namespace} \\\n    --set istio.enabled=true\n    # You can set ambassador instead with --set ambassador.enabled=true\n</code></pre>\n<h3>Check the namespace</h3>\n<pre><code>kubectl get ns\n</code></pre>\n<h2>Use docker image</h2>\n<pre><code>mlflow models build-docker -m runs:/&#x3C;run_id>/model -n &#x3C;your_dockerhub_user_name>/{image_tag} --enable-mlserver\n</code></pre>\n<h2>How to do it locally</h2>\n<p>First run docker desktop, then create a cluster using <code>kind</code>. Then, set up a seldon-core namespace.\nNext, build the model image.\nThen apply the config onto the namespace.</p>\n<pre><code>kubectl apply -f config.yaml\n</code></pre>\n<h2>Simple local deployment</h2>\n<p>After using <code>mlflow.log_model()</code>, the model will have some <code>model_uri</code>. Using this, you can deploy the model locally with the following command</p>\n<h3>Add pyenv</h3>\n<pre><code>curl https://pyenv.run | bash\npython -m  pip install virtualenv\n\nexport PATH=\"$HOME/.pyenv/bin:$PATH\"\n</code></pre>","frontmatter":{"date":"February 17, 2025","slug":"mlflow_on_kube","title":"Serving Machine Learning Model on Kubernetes"}}},"pageContext":{"id":"a6b3c4f8-368b-5228-b9bc-cecec636085c"}},"staticQueryHashes":[],"slicesMap":{}}